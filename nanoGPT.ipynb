{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QtquTqlaOel",
        "outputId": "d8d131e7-4968-4b86-f11c-2cd37b4d9792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-26 17:43:46--  https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-02-26 17:43:46 (18.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load text\n",
        "with open('input.txt') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyMZs63WaXG2",
        "outputId": "49029187-9f8d-4a1f-eecc-00ef963e2a0f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm64fqtrMCgP",
        "outputId": "121a10ec-6994-4814-9ac4-576d2d637db3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1832ff6670>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess text\n",
        "text = text.lower()\n",
        "\n",
        "# build vocab\n",
        "chars = sorted(list(set(text)))"
      ],
      "metadata": {
        "id": "IycEbFEranvL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 # number of instances to process simultaneously\n",
        "block_size = 256 # maximum context length allowed\n",
        "n_embed = 384\n",
        "n_heads = 6\n",
        "head_size = n_embed // n_heads\n",
        "n_layers = 6\n",
        "dropout = 0.2\n",
        "vocab_size = len(chars)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "learning_rate = 2e-4"
      ],
      "metadata": {
        "id": "lNWKFc0JLy7l"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define decoder & encoder\n",
        "stoi = {s:i for i, s in enumerate(chars)}\n",
        "itos = {i:s for i, s in enumerate(chars)}\n",
        "encode = lambda text: [stoi[s] for s in text]\n",
        "decode = lambda toks: ''.join([itos[i] for i in toks])"
      ],
      "metadata": {
        "id": "dAR_e_yga3GD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# encode text\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzSWT5M2cCbw",
        "outputId": "8cfd9a04-2cea-4b74-9ce9-3eb3f5696e6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1115394])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split data\n",
        "train_set_size = 0.9\n",
        "n = int(train_set_size*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "PazRcdJIbRVk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    if split not in ('train', 'val'):\n",
        "        raise Exception('split must be train or val')\n",
        "\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    high = len(data) - block_size\n",
        "    idxs = torch.randint(low=0, high=high, size=(batch_size,))\n",
        "    x = torch.stack([data[idx:idx+block_size] for idx in idxs])\n",
        "    y = torch.stack([data[idx+1:idx+block_size+1] for idx in idxs])\n",
        "\n",
        "    return x.to(device), y.to(device)"
      ],
      "metadata": {
        "id": "19niArsHdDp_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model performance\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_iters=1_000):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for i in range(eval_iters):\n",
        "            xb, yb = get_batch(split)\n",
        "            _, loss = model(xb, yb)\n",
        "            losses[i] = loss.item()\n",
        "        out[split] = losses.mean().item()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "vZXGI121zmbP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embed=n_embed, head_size=head_size, dropout=dropout) -> None:\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, T, C = x.shape\n",
        "        k = self.key(x) # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        weights = q @ k.transpose(-2, -1) * C**-0.5 # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
        "        weights = F.softmax(weights, dim=-1) # (B,T,T)\n",
        "        weights = self.dropout(weights)\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = weights @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "bIpvotDeKWlI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads=n_heads, head_size=head_size, dropout=dropout) -> None:\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([\n",
        "            SingleHeadAttention() for _ in range(n_heads)\n",
        "        ])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        x = self.proj(x)\n",
        "        out = self.dropout(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "qvETBUNANMh6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "    def __init__(self, dropout=dropout) -> None:\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "QB4dvJMLP4MD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed=n_embed, n_heads=n_heads):\n",
        "        super().__init__()\n",
        "        self.sa_heads = MultiHeadAttention(n_heads, head_size)\n",
        "        self.feed_forward = FeedForwardLayer()\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa_heads(self.ln1(x))\n",
        "        x = x + self.feed_forward(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "3AFUGjmZRxa1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLM(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.tok_embedding = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "        # for stabilizing training\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_embed = self.tok_embedding(idx) # (B,T,C)\n",
        "        pos_embed = self.position_embedding(\n",
        "            torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_embed + pos_embed # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.layer_norm(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # idx: (B, T), logits: (B, T, C)\n",
        "        B, T, C = logits.shape\n",
        "\n",
        "        if targets is None:\n",
        "            return logits, None\n",
        "\n",
        "        loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :] # (B,C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "zVCIPw19hK_r"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLM().to(device)\n",
        "xb, yb = get_batch('train')\n",
        "logits, loss = model(xb, yb)"
      ],
      "metadata": {
        "id": "g-vdVFtGoGPL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode(model.generate(torch.zeros((1, 1), dtype=torch.long).to(device),\n",
        "                   max_new_tokens=100)[0].tolist())"
      ],
      "metadata": {
        "id": "rPK2MX3xsCPl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "29699321-95ac-4281-fec1-76ac46b1cd58"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n-ekfxymgtcbpp::jg,x?mom,&be;tml?fv&aqmmcadua!pqsy\\n dass:m:-pr\\n!$wovr &nkg,f3sxz.u\\nzuy;oq,e;rm-\\nif-vo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "2jFzNhEasTjM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3_000\n",
        "for e in range(epochs):\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if e % 1_000 == 0:\n",
        "        print(loss.item())"
      ],
      "metadata": {
        "id": "Zfd_GOnDx4c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "309863b5-ca09-4ebc-c63b-b40824ee9e3f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0421206951141357\n",
            "1.0306305885314941\n",
            "1.0093374252319336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimate_loss(model,eval_iters=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE9Tu39zWzxG",
        "outputId": "c242a33f-4390-4dbc-a83d-f92ad90456d2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': 0.7893437147140503, 'val': 1.5771703720092773}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = decode(model.generate(torch.zeros((1, 1), dtype=torch.long).to(device),\n",
        "                   max_new_tokens=3000)[0].tolist())\n",
        "print(res)"
      ],
      "metadata": {
        "id": "d9WVpgvlyrq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc9df5e-5039-45d6-f621-c04ea06debe7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "and i will in evertae dull awthire?\n",
            "ay, is not at myself musit,--though revengeful\n",
            "like me forceful shouldst do that any man's!\n",
            "\n",
            "grumilio:\n",
            "tush, and no poison wep not with the foul cause.\n",
            "\n",
            "petruchio:\n",
            "a saint coil, you shall be well builted\n",
            "withal; to her it hath some that kill'd my aufidius.\n",
            "\n",
            "gentleman:\n",
            "beat you, to the watch, sit's a man droop\n",
            "to looking the severerely of your voices!\n",
            "\n",
            "clarence:\n",
            "titus:\n",
            "you veronation is modest for this kind of contrary,\n",
            "and merited to romeo, you mistake me\n",
            "a lottendary of the body.\n",
            "\n",
            "both:\n",
            "mopsa, it was; therefore hereforw or incords,\n",
            "i came it to breather.\n",
            "\n",
            "romeo:\n",
            "why, that's well: go wink, he is an rosema: it\n",
            "so. there's none can pity to the state,\n",
            "and many better subjects, which they royalty\n",
            "he is asking to our furreson stainly beams\n",
            "and to heaven there still wherein the hearts\n",
            "which poestern our souls, he'ld wear us.\n",
            "\n",
            "paulina:\n",
            "let me be reason'd;\n",
            "for so that my knees how he is not king,\n",
            "it cannot help but will for revenge it,\n",
            "like one confound to their but imports,\n",
            "and they to any more\n",
            "than the under manal is doings of night.\n",
            "but reasonate crave when thou dost do it,\n",
            "from thee to affect on thy life i'll tear;\n",
            "exceptaure our decalle whose grave, the room of him\n",
            "aim'd with ravens. now lords, angelo his friends,\n",
            "and, that with my uncle gaunt, be an humble angerser,\n",
            "but send him to the king, yet his throat chin\n",
            "done any doom: shall i then?\n",
            "yet whether were bastards dare no less,\n",
            "that makes him wull affected a side.\n",
            "\n",
            "prince:\n",
            "god save your limbs where expeditied and time!\n",
            "\n",
            "juliet:\n",
            "in government, ladiers: forbear warns i see\n",
            "with what you have been at the minister.\n",
            "3 kind henry vi\n",
            "\n",
            "first watchman:\n",
            "he that hath been brief an exchalent,\n",
            "which, in first, she deputy, like aspirite,\n",
            "which so is pastly peasing doth wrought unto the drops,\n",
            "and but aspect of titus, she is dead,\n",
            "my most days in justice.\n",
            "\n",
            "clifford:\n",
            "every toy of thy heaviest, say him,\n",
            "so he will inclined oranotive, that\n",
            "i have conjuratcts to him, and in a mourse\n",
            "to use him and aus poison, profeters and bread;\n",
            "we'll all this sforreign so lout our tribunes pill.\n",
            "\n",
            "antigonus:\n",
            "be it willy reasure her:\n",
            "if well the king, this is sistent kind\n",
            "where is banishme?\n",
            "\n",
            "perdita:\n",
            "if them did my is restoring of foul points\n",
            "keeps to his neck or this deed to the scars\n",
            "that which doth command, my worse's spoil.\n",
            "\n",
            "florizel:\n",
            "i have from you'brty, sir, for pityiness powder,\n",
            "you speak from the virtue of banishment, is\n",
            "contradict to pity me secrecy rested fallegually.\n",
            "\n",
            "brutus:\n",
            "so i take my royal polixenes,\n",
            "from what dead is ours distressing, flatters\n",
            "into in our half-pints, and stirriketchless.\n",
            "pileaser of what\n",
            "have life is with charged this possemble strike\n",
            "all half plaated in thee to masks a well,\n",
            "destroy'd and toker still sleep, 'fore pierce's limb;\n",
            "which like the numbers small set dogs, and suffer.\n",
            "ifs thou she'st--as the friend months her come\n",
            "to grieve his son: what patiency gives creature\n",
            "persuades not, are run in about,\n",
            "balest high-ring; who are great, and are going\n",
            "would, our s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"out.txt\", \"w\") as file:\n",
        "    file.write(res)"
      ],
      "metadata": {
        "id": "2gkr4qRYXPjE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.save(model.state_dict(), \"model_weights.pth\")"
      ],
      "metadata": {
        "id": "sxWbLzVHffH6"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}